---
title: 反向传播算法
categories:
  - AI算法
---
## BPTT

> BPTT: BackPropagation Through Time (通过时间反向传播)

**通过时间反向传播 (BPTT)** 是一种训练循环神经网络 (RNN) 的技术。基本上，它涉及到将循环神经网络展开成为一个深度神经网络，并按照这种展开的网络进行反向传播。

### 误差信号


误差信号 $v(t)$ 的定义基于链式法则（chain rule）的概念，为了计算它，我们首先需要回顾神经网络的误差函数以及如何通过反向传播来计算权重的梯度。

#### 输出单元的误差信号

对于神经网络的一个输出单元，我们使用均方误差作为损失函数：

$$E(t)=\frac{1}{2}\left(d_{k}(t)-y^{k}(t)\right)^{2}$$

其中:
- $E(t)$ 是在时刻 t 的误差。
- $d_{k}(t)$ 是目标输出。
- $y^k(t)$ 是神经网络的实际输出。

我们想要找到损失函数 $E(t)$ 相对于净输入 $net_k(t)$ 的导数，因为这会告诉我们如何调整权重以减小误差。

在时刻 t，**输出单元** k 的目标由 $d_k(t)$ 表示。使用均方误差，k 的误差信号为 $v(t)$，其定义为：

$$v(t) = \frac{\partial E(t)}{\partial net_{k}(t)}=\frac{\partial E(t)}{\partial y^{k}(t)} \times \frac{\partial y^{k}(t)}{\partial net_{k}(t)}$$

- $net_{k}(t)$ 是 **输出单元** 的净输入
- $y^k(t)$ 是 **输出单元** 的实际输出。

所以最终我们得到误差信号表达式：

$$v_k(t) = f'_k(net_k(t))(y^k(t)-d_k(t))$$

#### 非输出单元的误差信号

首先复习链式法则的多变量版本，也叫做**多元链式法则（multivariate chain rule）**

多元链式法则可以这样描述：考虑一个函数 $y(u_1, u_2, u_3, ..., u_n)$，其中每个$u_i$都是另一个变量$x_i$的函数。即$u_i = g_i(x_1, x_2, x_3, ..., x_m)$，那么y关于的偏导数是：

$$
\frac{\partial y}{\partial x_{j}}=\sum_{i=1}^{m} \frac{\partial y}{\partial u_{i}} \frac{\partial u_{i}}{\partial x_{j}}
$$

在神经网络的背景下，对于一个非输出节点$j$，最终的 Loss 函数可以通过所有与 $j$ 相连的下一层（离输出层更近的那一层）中的节点$i$反向传播到$j$


$$
    v_{j} = \frac{\partial E}{\partial net_{j}} = \sum_{i} \frac{\partial E}{\partial net_{i}} \frac{\partial net_{i}}{\partial net_{j}}
$$

其中

$$
    \frac{\partial E}{\partial net_i{i}} = v_i 
$$


$$
    \frac{\partial net_{i}}{\partial net_{j}} = \frac{\partial net_{i}}{\partial out_{j}} \frac{\partial out_j}{\partial net_{j}}
$$

根据上面公式, 不难推出：


$$v_{j}=f_{j}^{\prime}\left(net_{j}\right) \sum_{i} w_{i j} v_{i}$$

这就是非输出单元误差信号的求法

### 梯度下降

我们有了每一个节点的误差信号，对于任意一个节点$i$与$j$之间的权重$w_{ij}$，我们需要的关键数据如下：

$$\frac{\partial \text{loss}}{\partial w_{ij}(t)} = y^i(t) \times v_j(t + 1)$$

可以看到，我们求出每个节点的误差信号后，就可以非常简单的运行反向传播中的梯度下降
